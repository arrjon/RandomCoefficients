{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from typing import Optional\n",
    "from tqdm.auto import tqdm\n",
    "import numdifftools as nd\n",
    "from functools import partial\n",
    "\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scienceplots\n",
    "plt.style.use(['science'])\n",
    "plt.rcParams.update({#'text.usetex' : True,\n",
    "                     'font.size': 25,\n",
    "                     #'font.family' : 'serif',\n",
    "                     #'mathtext.fontset':  ,\n",
    "                     'axes.titlesize': 'x-small',\n",
    "                     'axes.labelsize': 'xx-small',\n",
    "                     'xtick.labelsize': 'xx-small',\n",
    "                     'ytick.labelsize': 'xx-small',\n",
    "                     'legend.fontsize': 'xx-small',\n",
    "                     #'figure.dpi': 600,\n",
    "                     'figure.figsize': (8,6)})\n",
    "#colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@njit\n",
    "def g(product_ij: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "    inside_prod = np.exp(product_ij.dot(beta)) / (1 + np.sum(np.exp(product_ij.dot(beta))))\n",
    "    outside_prod = 1 - np.sum(inside_prod)\n",
    "    return np.append(outside_prod, inside_prod)\n",
    "\n",
    "def generate_data(gaussian_means: np.ndarray,\n",
    "                  gaussian_sigmas: np.ndarray,\n",
    "                  gaussian_mixing_probs: np.ndarray,\n",
    "                  n_consumers: int,\n",
    "                  seed: int = 42) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    assert gaussian_mixing_probs.shape[0] == gaussian_means.shape[0] == gaussian_sigmas.shape[0]\n",
    "    assert gaussian_means.shape[1] == gaussian_sigmas.shape[1]\n",
    "    n_product_attributes = gaussian_means.shape[1]\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Generate independent variable (two products with two attributes each)\n",
    "    ## J=2\n",
    "    products = np.stack((np.random.uniform(0, 5, size=(n_consumers, n_product_attributes)),\n",
    "                         np.random.uniform(-3, 1, size=(n_consumers, n_product_attributes))), axis=2)\n",
    "\n",
    "    #x_i = np.array([[1, 2],  #j=1\n",
    "    #                 [1.5, 1.5]])  # j=2\n",
    "    # does not vary with consumer\n",
    "    #products = np.repeat(x_i[np.newaxis], n, axis=0)\n",
    "\n",
    "    beta = np.zeros((n_consumers, products.shape[2]))\n",
    "    for mix_i, prob in enumerate(gaussian_mixing_probs):\n",
    "        beta += prob*np.random.multivariate_normal(mean=gaussian_means[mix_i],\n",
    "                                                   cov=gaussian_sigmas[mix_i],\n",
    "                                                   size=n_consumers)\n",
    "\n",
    "    # Generate dependent variable\n",
    "    y_prop = np.zeros((n_consumers, products.shape[1]+1))\n",
    "    y = np.zeros(n)\n",
    "    for consumer_id in range(n_consumers):\n",
    "        y_prop[consumer_id] = g(products[consumer_id, :], beta[consumer_id])\n",
    "        y[consumer_id] = np.random.choice(np.arange(products.shape[1]+1), p=y_prop[consumer_id])\n",
    "\n",
    "    # expand y into binary data frame\n",
    "    choices = np.zeros((n_consumers, products.shape[1]+1))\n",
    "    for i in range(products.shape[1]+1):\n",
    "        choices[y == i, i] = 1\n",
    "    return products, beta, choices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{align*}\n",
    "    P_{i,j}(x) = \\int \\frac{\\exp (x^T_{i,j} \\beta)}{1 + \\sum_{l=1}^J \\exp(x^T_{i,l} \\beta)} \\, dF(\\beta)\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# number of observations\n",
    "n = 1000\n",
    "# true mixture distribution\n",
    "means = np.array([[-2.2, -2.2], [1.3, 1.3]])\n",
    "sigmas = np.array([[[0.8, 0.15], [0.15, 0.8]], [[0.8, 0.15], [0.15, 0.8]]])\n",
    "mixing_probs = np.array([0.5, 0.5])\n",
    "\n",
    "products, beta_unobserved, choiceData = generate_data(gaussian_means=means,\n",
    "                                                      gaussian_sigmas=sigmas,\n",
    "                                                      gaussian_mixing_probs=mixing_probs,\n",
    "                                                      n_consumers=n)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@njit\n",
    "def eval_beta_grid(depended_y: np.ndarray, explanatory_x: np.ndarray, grid_vector: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    n_grid_points = grid_vector.shape[0]\n",
    "    n_consumers, n_products, _ = explanatory_x.shape\n",
    "\n",
    "    tilde_z = np.zeros((n_consumers, n_products+1, n_grid_points-1))\n",
    "    tilde_y = np.zeros((n_consumers, n_products+1))\n",
    "    for consumer_ind in range(n_consumers):\n",
    "        z_r = g(explanatory_x[consumer_ind], grid_vector[-1])\n",
    "        for b_r, beta_r in enumerate(grid_vector[:-1]):\n",
    "            tilde_z[consumer_ind, :, b_r] = g(explanatory_x[consumer_ind], beta_r) - z_r\n",
    "        tilde_y[consumer_ind] = depended_y[consumer_ind] - z_r\n",
    "\n",
    "    # just a scaling factor, influences the size of mu, was not used in paper\n",
    "    factor = 1 #/np.sqrt(n_consumers*n_products)\n",
    "    tilde_z = tilde_z.reshape(n_consumers*(n_products+1), n_grid_points-1) * factor\n",
    "    tilde_y = tilde_y.reshape(n_consumers*(n_products+1)) * factor\n",
    "\n",
    "    return tilde_z, tilde_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_quadratic_matrices(depended_y: np.ndarray, explanatory_x: np.ndarray,\n",
    "                              grid_vector: np.ndarray,\n",
    "                              mu_penalty: float = 0.) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"takes data, explanatory variables, a grid and a penalty and build matrices for quadratic problem\"\"\"\n",
    "    n_grid_points = grid_vector.shape[0]\n",
    "    assert explanatory_x.shape[2] == grid_vector.shape[1]\n",
    "\n",
    "    tilde_z, tilde_y = eval_beta_grid(depended_y=depended_y, explanatory_x=explanatory_x, grid_vector=grid_vector)\n",
    "\n",
    "    # build matrices for quadratic optimization\n",
    "    P = tilde_z.T.dot(tilde_z)\n",
    "    q = -tilde_z.T.dot(tilde_y)\n",
    "\n",
    "    # add quadratic constraint\n",
    "    if mu_penalty > 0:\n",
    "        P += np.diag(np.ones(n_grid_points-1)*mu_penalty)  # todo: maybe move somewhere else to make cv more efficient\n",
    "    return P, q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_beta_grid(n_grid_points: int,\n",
    "                   dim: int,\n",
    "                   support: tuple[float, float],\n",
    "                   sampling_type: str) -> np.ndarray:\n",
    "    \"\"\"returns a vector of shape (n_grid_points, dim)\"\"\"\n",
    "    if sampling_type == 'halton':\n",
    "        # not random, only due to scrambling\n",
    "        h_sampler = scipy.stats.qmc.Halton(d=dim, scramble=True)\n",
    "        beta_grid_01 = h_sampler.random(n_grid_points)\n",
    "    elif sampling_type == 'sobol':\n",
    "        # not random, only due to scrambling\n",
    "        h_sampler = scipy.stats.qmc.Sobol(d=dim, scramble=True)\n",
    "        beta_grid_01 = h_sampler.random(n_grid_points)\n",
    "    elif sampling_type == 'latin_hypercubes':\n",
    "        # Latin hyper-cube sampling maintains random samples at it core\n",
    "        lh_sampler = scipy.stats.qmc.LatinHypercube(d=dim, scramble=True)\n",
    "        beta_grid_01 = lh_sampler.random(n_grid_points)\n",
    "    elif sampling_type == 'latin_hypercubes_2':\n",
    "        # Halton is more performant than orthogonal hypercubes (based on scipy documentation)\n",
    "        # Latin hyper-cube sampling maintains random samples at it core\n",
    "        lh_sampler = scipy.stats.qmc.LatinHypercube(d=dim, scramble=True, strength=2)\n",
    "        beta_grid_01 = lh_sampler.random(n_grid_points)\n",
    "    elif sampling_type == 'uniform':\n",
    "        beta_grid_01 = np.random.uniform(size=(n_grid_points, dim))\n",
    "    else:\n",
    "        raise NotImplementedError('sampling_type not implemented')\n",
    "    grid = scipy.stats.qmc.scale(beta_grid_01, np.ones(dim)*support[0], np.ones(dim)*support[1])\n",
    "    return grid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constraints\n",
    "$$ Gx \\leq h$$\n",
    "$$ \\theta_r \\geq 0 \\quad\\text{and}\\quad \\sum_{r=1}^{R-1} \\theta_r \\leq 1$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_non_parametric(data_y: np.ndarray,\n",
    "                         data_x: np.ndarray,\n",
    "                         grid: Optional[np.ndarray] = None,\n",
    "                         n_grid_points: Optional[int] = None,\n",
    "                         grid_sampling_type: str = 'halton',\n",
    "                         mu_penalty: float = 0.,\n",
    "                         eps: float = 1e-8,\n",
    "                         verbose = False):\n",
    "    \"\"\"takes data and optionally grid and solves for the distribution of the random effects\"\"\"\n",
    "    if grid is None and n_grid_points is not None:\n",
    "        grid = make_beta_grid(n_grid_points=n_grid_points, dim=products.shape[2], support=(-4.5, 3.5),\n",
    "                              sampling_type=grid_sampling_type)\n",
    "    elif grid is None and n_grid_points is None:\n",
    "        raise ValueError('Either grid or n_grid_points must be specified')\n",
    "\n",
    "    P, q = create_quadratic_matrices(depended_y=data_y, explanatory_x=data_x, grid_vector=grid, mu_penalty=mu_penalty)\n",
    "\n",
    "    # inequality constraints\n",
    "    # non-negativity and l1-penalty\n",
    "    G = np.concatenate((-np.eye((grid.shape[0]-1)), np.ones((1, grid.shape[0]-1))))\n",
    "    h = np.concatenate((np.zeros(grid.shape[0]-1), [1]))\n",
    "\n",
    "    # solve\n",
    "    solvers.options['show_progress'] = verbose\n",
    "    sol = solvers.qp(P=cvxopt_matrix(P), q=cvxopt_matrix(q), G=cvxopt_matrix(G), h=cvxopt_matrix(h), A=None, b=None)\n",
    "\n",
    "    # get solution and cut off minor weights\n",
    "    theta_raw = np.array(sol['x']).flatten()\n",
    "    boolean_index = theta_raw > eps\n",
    "    theta = theta_raw * boolean_index\n",
    "    theta = np.append(theta, 1-theta.sum())\n",
    "    boolean_index = np.append(boolean_index, True)\n",
    "    sparse_index = np.arange(0, theta.size)[boolean_index]\n",
    "\n",
    "    if verbose:\n",
    "        print('number of negative weights:', np.sum(np.array(h) - np.array(G).dot(np.array(sol['x'])).T < 0))\n",
    "        print('non-zero weights:', sparse_index.size)\n",
    "        print('total number of weights:', theta.size)\n",
    "    return theta, grid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@njit\n",
    "def estimated_cdf_grid(x_grid: np.ndarray, y_grid: np.ndarray, grid: np.ndarray, thetas: np.ndarray) -> np.ndarray:\n",
    "    n_evals = x_grid.shape[0]\n",
    "    cdf_out = np.zeros((n_evals, n_evals))\n",
    "    for y_i in range(n_evals):\n",
    "        for x_i in range(n_evals):\n",
    "            eval_points = np.array([x_grid[x_i, y_i], y_grid[x_i, y_i]])\n",
    "            for b_id, beta in enumerate(grid):\n",
    "                if (eval_points > beta).all():\n",
    "                    cdf_out[x_i, y_i] += thetas[b_id]\n",
    "    return cdf_out\n",
    "\n",
    "@njit\n",
    "def estimated_cdf_vector(x_vec: np.ndarray, grid: np.ndarray, thetas: np.ndarray) -> np.ndarray:\n",
    "    cdf_out = np.zeros_like(x_vec)\n",
    "    for x_i in range(x_vec.shape[0]):\n",
    "        for b_id, beta in enumerate(grid):\n",
    "            if (x_vec[x_i] > beta).all():\n",
    "                cdf_out[x_i] += thetas[b_id]\n",
    "    return cdf_out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval_gaussian_grid(x_grid: np.ndarray, y_grid: np.ndarray,\n",
    "                       gaussian_means: np.ndarray, gaussian_covs: np.ndarray, gaussian_mixing_coeff: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"takes a 2D-meshgrid and evaluates a gaussian mixture's pdf and cdf on it\"\"\"\n",
    "    gaussians = []\n",
    "    for c_i, coeff in enumerate(gaussian_mixing_coeff):\n",
    "        gaussians.append(scipy.stats.multivariate_normal(mean=gaussian_means[c_i], cov=gaussian_covs[c_i]))\n",
    "    n_evals = x_grid.shape[0]\n",
    "    cdf_out = np.zeros((n_evals, n_evals, 2))\n",
    "    for x_i in range(n_evals):\n",
    "        x_val = np.stack((x_grid[x_i, :], y_grid[x_i, :])).T\n",
    "        cdf_out[:, x_i, 0] = np.sum([coeff*gaussian.pdf(x_val)\n",
    "                                     for coeff, gaussian in zip(gaussian_mixing_coeff, gaussians)], axis=0)\n",
    "        cdf_out[:, x_i, 1] = np.sum([coeff*gaussian.cdf(x_val)\n",
    "                                     for coeff, gaussian in zip(gaussian_mixing_coeff, gaussians)], axis=0)\n",
    "    return cdf_out\n",
    "\n",
    "def eval_gaussian_vector(x_full: np.ndarray,\n",
    "                         gaussian_means: np.ndarray, gaussian_covs: np.ndarray, gaussian_mixing_coeff: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"takes a vector of points and evaluates a gaussian mixture's cdf on it\"\"\"\n",
    "    gaussians = []\n",
    "    for c_i, coeff in enumerate(gaussian_mixing_coeff):\n",
    "        gaussians.append(scipy.stats.multivariate_normal(mean=gaussian_means[c_i], cov=gaussian_covs[c_i]))\n",
    "    cdf_out = np.zeros_like(x_full)\n",
    "    for x_id, x_i in enumerate(x_full):\n",
    "        cdf_out[x_id] = np.sum([coeff*gaussian.cdf(x_i) for coeff, gaussian in zip(gaussian_mixing_coeff, gaussians)], axis=0)\n",
    "    return cdf_out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "theta_test, beta_grid_test = solve_non_parametric(data_y=choiceData, data_x=products,\n",
    "                                                  n_grid_points=100, mu_penalty=0, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot beta grid\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(beta_grid_test[:, 0], beta_grid_test[:, 1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# estimate cdf\n",
    "x = np.linspace(-4.5, 3.5, 100)\n",
    "y = np.linspace(-4.5, 3.5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z_estimated = estimated_cdf_grid(X, Y, grid=beta_grid_test, thetas=theta_test)\n",
    "Z_real = eval_gaussian_grid(X, Y, gaussian_means=means, gaussian_covs=sigmas, gaussian_mixing_coeff=mixing_probs)\n",
    "mean_squared_error = np.mean((Z_real[:, :, 1]-Z_estimated)**2)\n",
    "mean_squared_error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cross_validate(data_x: np.ndarray, data_y: np.ndarray,\n",
    "                   mu_list: list, beta_grid: np.ndarray, kfold: int = 3,\n",
    "                   compute_real_error: bool = False,\n",
    "                   verbose=True):\n",
    "    assert kfold >= 2\n",
    "    if compute_real_error:\n",
    "        xs = np.linspace(-4.5, 3.5, 1000)\n",
    "        ys = np.linspace(-4.5, 3.5, 1000)\n",
    "        x_grid, y_grid = np.meshgrid(xs, ys)\n",
    "        real_cdf = eval_gaussian_grid(x_grid=x_grid, y_grid=y_grid,\n",
    "                                      gaussian_means=means, gaussian_covs=sigmas, gaussian_mixing_coeff=mixing_probs)\n",
    "        real_error = np.zeros((len(mu_list), kfold))\n",
    "\n",
    "    cross_valid_error = np.zeros((len(mu_list), kfold))\n",
    "    weights = np.zeros((len(mu_list), kfold, beta_grid.shape[0]))\n",
    "\n",
    "    x_chunks = np.array_split(data_x, kfold)\n",
    "    y_chunks = np.array_split(data_y, kfold)\n",
    "\n",
    "    for mu_i in tqdm(range(len(mu_list)), desc='$\\mu$', disable=not verbose):\n",
    "        for k in range(kfold):\n",
    "            test_data_chunk = np.concatenate((x_chunks[:k] + x_chunks[k+1:]), axis=0)\n",
    "            test_product_chunk = np.concatenate((y_chunks[:k] + y_chunks[k+1:]), axis=0)\n",
    "\n",
    "            theta_i_k, _ = solve_non_parametric(data_y=test_product_chunk, data_x=test_data_chunk,\n",
    "                                                grid=beta_grid, mu_penalty=mu_list[mu_i], eps=1e-8)\n",
    "            weights[mu_i, k] = theta_i_k\n",
    "\n",
    "            # compute cross validation error\n",
    "            tilde_z, tilde_y = eval_beta_grid(depended_y=y_chunks[k], explanatory_x=x_chunks[k],\n",
    "                                              grid_vector=beta_grid)\n",
    "            cross_valid_error[mu_i, k] = np.mean((tilde_z.dot(theta_i_k[:-1]) - tilde_y)**2)\n",
    "\n",
    "            # compute error to true cdf\n",
    "            if compute_real_error:\n",
    "                estimate_cdf = estimated_cdf_grid(x_grid=x_grid, y_grid=y_grid,\n",
    "                                                  grid=beta_grid, thetas=theta_i_k)\n",
    "                real_error[mu_i, k] = np.mean((real_cdf[:, :, 1]-estimate_cdf)**2)\n",
    "    if compute_real_error:\n",
    "        return weights, cross_valid_error, real_error\n",
    "    return weights, cross_valid_error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu_list_test = np.linspace(0, 100, 10)  # mu in paper are reported in the order 50 / n / products.shape[1]\n",
    "#mu_list_test = np.append(mu_list_test, [0.2, 0.5, 1])\n",
    "R = 100\n",
    "k_folds = 2\n",
    "print(mu_list_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weights_list, cross_validation_error, error_true = cross_validate(data_x=products, data_y=choiceData,\n",
    "                                                                  mu_list=mu_list_test,\n",
    "                                                                  beta_grid=beta_grid_test,\n",
    "                                                                  kfold=k_folds,\n",
    "                                                                  compute_real_error=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_cv_mu(mu_list_test: np.ndarray, cross_validation_error: np.ndarray) -> (float, float):\n",
    "    k_folds = cross_validation_error.shape[1]\n",
    "    cross_valid_error_mean = np.mean(cross_validation_error, axis=1)\n",
    "    mu_index = np.nanargmin(cross_valid_error_mean)\n",
    "    mu_MSE = mu_list_test[mu_index]\n",
    "\n",
    "    cvStandardError = np.std(cross_validation_error, axis=1) / np.sqrt(k_folds)\n",
    "    allowed_mu = cross_valid_error_mean <= cross_valid_error_mean[mu_index] + cvStandardError[mu_index]\n",
    "    muOneSe = np.max(mu_list_test[allowed_mu])  # todo: why max\n",
    "    return mu_MSE, muOneSe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu_MSE, muOneSe = compute_cv_mu(mu_list_test, cross_validation_error)\n",
    "print('best $\\mu$ cross validated (MSE):', mu_MSE)\n",
    "print('best $\\mu$ cross validated (OneSE):', muOneSe)\n",
    "\n",
    "true_error_mean = np.mean(error_true, axis=1)\n",
    "best_mu = np.nanargmin(true_error_mean)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cross_valid_error_mean = np.mean(cross_validation_error, axis=1)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(mu_list_test, true_error_mean, label='True error')\n",
    "plt.scatter(mu_list_test, cross_valid_error_mean, label='Cross Validation error')\n",
    "plt.scatter(mu_list_test[np.nanargmin(cross_valid_error_mean)], np.nanmin(cross_valid_error_mean),\n",
    "            label=f'MSE $\\mu={mu_list_test[np.nanargmin(cross_valid_error_mean)].round(2)}$')\n",
    "plt.scatter(muOneSe, cross_valid_error_mean[np.argmax(muOneSe == np.array(mu_list_test))],\n",
    "            label=f'OneSE $\\mu={muOneSe.round(2)}$')\n",
    "#plt.scatter(mu_list_test[best_mu], cross_valid_error_mean[best_mu],\n",
    "#            label=f'true minimal error $\\mu={mu_list_test[best_mu]}$', marker='x')\n",
    "plt.yscale('log')\n",
    "plt.title('Cross validation error')\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(cross_valid_error_mean)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_weights = np.zeros(len(mu_list_test))\n",
    "for mu_idx, mu in enumerate(mu_list_test):\n",
    "    n_weights[mu_idx] = np.mean(np.sum(weights_list[mu_idx] > 1e-5, axis=1))\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(mu_list_test, n_weights)\n",
    "plt.title('Number of weights')\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('Number of weights')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "theta, beta_grid = solve_non_parametric(choiceData, products,\n",
    "                                        grid=beta_grid_test,\n",
    "                                        #n_grid_points=R,\n",
    "                                        mu_penalty=1, #mu_MSE\n",
    "                                verbose=True)\n",
    "Z_estimated = estimated_cdf_grid(X, Y, grid=beta_grid, thetas=theta)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot cdf\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(X, Y, Z_estimated, cmap='cool', label='estimated')\n",
    "\n",
    "ax.set_title(f'Estimated CDF with error={np.mean((Z_real[:, :, 1]-Z_estimated)**2).round(5)}', fontsize=14)\n",
    "ax.set_xlabel('attribute 1', fontsize=12)\n",
    "ax.set_ylabel('attribute 2', fontsize=12)\n",
    "ax.set_zlabel('z', fontsize=12)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot difference cdf\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(X, Y, (Z_real[:, :, 1]-Z_estimated)**2, cmap='cool', label='cdf')\n",
    "\n",
    "#ax.set_title('Squared Difference to True CDF', fontsize=14)\n",
    "ax.set_xlabel('attribute 1', fontsize=12)\n",
    "ax.set_ylabel('attribute 2', fontsize=12)\n",
    "ax.set_zlabel('z', fontsize=12)\n",
    "ax.set_zlim(0,0.1)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot cdf\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(X, Y, Z_real[:, :, 1], cmap='cool', label='cdf')\n",
    "\n",
    "ax.set_title('Real CDF', fontsize=14)\n",
    "ax.set_xlabel('attribute 1', fontsize=12)\n",
    "ax.set_ylabel('attribute 2', fontsize=12)\n",
    "ax.set_zlabel('z', fontsize=12)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(X, Y, Z_real[:, :, 0], cmap='cool', label='pdf')\n",
    "\n",
    "ax.set_title(f'Real PDF with mean={means}', fontsize=14)\n",
    "ax.set_xlabel('attribute 1', fontsize=12)\n",
    "ax.set_ylabel('attribute 2', fontsize=12)\n",
    "ax.set_zlabel('z', fontsize=12)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# compute pdf from cdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@njit\n",
    "def estimated_cdf_pointwise(eval_point: np.ndarray, grid: np.ndarray, thetas: np.ndarray) -> float:\n",
    "    cdf_out = 0\n",
    "    for b_id, beta in enumerate(grid):\n",
    "        if (eval_point > beta).all():\n",
    "            cdf_out += thetas[b_id]\n",
    "    return cdf_out\n",
    "\n",
    "helper_cdf = partial(estimated_cdf_pointwise, grid=beta_grid, thetas=theta)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_pdf(x_grid, y_grid):\n",
    "    assert x_grid.shape == y_grid.shape\n",
    "\n",
    "    pdf = np.zeros((len(x_grid), len(y_grid)))\n",
    "    hessian = nd.Hessian(helper_cdf)  # derivative in both directions needed\n",
    "\n",
    "    for d1 in range(x_grid.shape[0]):\n",
    "        for d2 in range(x_grid.shape[1]):\n",
    "            h = hessian([x_grid[d1,d2], y_grid[d1,d2]])\n",
    "            pdf[d1, d2] = h[0,1]\n",
    "    return pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "pdf_estimate = compute_pdf(X, Y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "#ax.plot_surface(X, Y, Z_real[:, :, 0], cmap='cool', label='pdf')\n",
    "ax.plot_surface(X, Y, pdf_estimate, cmap='cool', label='pdf estimate')\n",
    "\n",
    "ax.set_title(f'Estimated PDF', fontsize=14)\n",
    "ax.set_xlabel('attribute 1', fontsize=12)\n",
    "ax.set_ylabel('attribute 2', fontsize=12)\n",
    "ax.set_zlabel('z', fontsize=12)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Estimate Moments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_moment(grid: np.ndarray, thetas: np.ndarray, n_moments: tuple):\n",
    "    assert grid.shape[1] == len(n_moments), 'you must provide a tuple with as many entries as dimension of coefficients'\n",
    "\n",
    "    grid_pow = np.power(grid, n_moments)\n",
    "    moment = grid_pow.prod(axis=1).dot(thetas)\n",
    "    return moment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert np.isclose(compute_moment(beta_grid_test, theta_test, n_moments=(0, 0)), 1), '0th moment should be 1'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "estimated_mean = np.array([compute_moment(beta_grid_test, theta_test, n_moments=(1, 0)),\n",
    "                           compute_moment(beta_grid_test, theta_test, n_moments=(0, 1))])\n",
    "\n",
    "estimated_var = np.array([compute_moment(beta_grid_test, theta_test, n_moments=(2, 0)),\n",
    "                          compute_moment(beta_grid_test, theta_test, n_moments=(0, 2))]) - estimated_mean**2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "real_sample_mean = np.mean(beta_unobserved, axis=0)\n",
    "real_sample_var = np.var(beta_unobserved, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(estimated_mean, real_sample_mean)\n",
    "print(estimated_var, real_sample_var)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5), tight_layout=True)\n",
    "\n",
    "plt.fill_between(np.arange(beta_unobserved.shape[0]),\n",
    "                estimated_mean[0]-1.98*np.sqrt(estimated_var[0]),  estimated_mean[0]+1.98*np.sqrt(estimated_var[0]),\n",
    "                 alpha=0.2, label='estimated variance 95\\% bound')\n",
    "\n",
    "plt.scatter(x=np.arange(beta_unobserved.shape[0]), y=beta_unobserved[:, 0], label='samples')\n",
    "plt.hlines(real_sample_mean[0], xmin=0, xmax=beta_unobserved.shape[0], color='red', label='sample mean')\n",
    "plt.hlines(estimated_mean[0], xmin=0, xmax=beta_unobserved.shape[0], color='orange', label='estimated mean')\n",
    "\n",
    "# plt fill between var 95% confidence interval\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,5), tight_layout=True)\n",
    "\n",
    "plt.fill_between(np.arange(beta_unobserved.shape[0]),\n",
    "                estimated_mean[1]-1.98*np.sqrt(estimated_var[1]),  estimated_mean[1]+1.98*np.sqrt(estimated_var[1]),\n",
    "                 alpha=0.2, label='estimated variance 95\\% bound')\n",
    "\n",
    "plt.scatter(x=np.arange(beta_unobserved.shape[0]), y=beta_unobserved[:, 1], label='samples')\n",
    "plt.hlines(real_sample_mean[1], xmin=0, xmax=beta_unobserved.shape[0], color='red', label='sample mean')\n",
    "plt.hlines(estimated_mean[1], xmin=0, xmax=beta_unobserved.shape[0], color='orange', label='estimated mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Sampling Methods"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T12:09:58.622017Z",
     "start_time": "2023-06-25T12:09:58.612602Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "beta_grid_test_uniform = make_beta_grid(n_grid_points=128, dim=products.shape[2], support=(-4.5, 3.5),\n",
    "                                        sampling_type='uniform')\n",
    "beta_grid_test_latin = make_beta_grid(n_grid_points=128, dim=products.shape[2], support=(-4.5, 3.5),\n",
    "                                        sampling_type='latin_hypercubes')\n",
    "beta_grid_test_latin_2 = make_beta_grid(n_grid_points=121, dim=products.shape[2], support=(-4.5, 3.5),\n",
    "                                        sampling_type='latin_hypercubes_2')\n",
    "beta_grid_test_sobol = make_beta_grid(n_grid_points=128, dim=products.shape[2], support=(-4.5, 3.5),\n",
    "                                        sampling_type='sobol')\n",
    "beta_grid_test_halton = make_beta_grid(n_grid_points=128, dim=products.shape[2], support=(-4.5, 3.5),\n",
    "                                        sampling_type='halton')\n",
    "x = np.linspace(-4.5, 3.5, 11)\n",
    "y = np.linspace(-4.5, 3.5, 11)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(15, 7))\n",
    "\n",
    "ax[0,0].set_title('Uniform Sampling')\n",
    "ax[0,0].scatter(beta_grid_test_uniform[:, 0], beta_grid_test_uniform[:, 1])\n",
    "\n",
    "ax[0,1].set_title('Latin Hypercubes Sampling')\n",
    "ax[0,1].scatter(beta_grid_test_latin[:, 0], beta_grid_test_latin[:, 1])\n",
    "\n",
    "ax[0,2].set_title('Latin Hypercubes Sampling with Strength 2')\n",
    "ax[0,2].scatter(beta_grid_test_latin_2[:, 0], beta_grid_test_latin_2[:, 1])\n",
    "\n",
    "ax[1,0].set_title('Sobol Sampling')\n",
    "ax[1,0].scatter(beta_grid_test_sobol[:, 0], beta_grid_test_sobol[:, 1])\n",
    "\n",
    "ax[1,1].set_title('Halton Sampling')\n",
    "ax[1,1].scatter(beta_grid_test_halton[:, 0], beta_grid_test_halton[:, 1])\n",
    "\n",
    "for x_i in x:\n",
    "    ax[1,2].set_title('Equal Distance')\n",
    "    ax[1,2].scatter(np.ones_like(y)*x_i, y, color='#1f77b4')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monte Carlo Simulations with fixed list of $\\mu$'s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "method_list = ['uniform', 'latin_hypercubes', 'latin_hypercubes_2', 'sobol', 'halton']\n",
    "n_grid_points_list = np.array([128, 128, 121, 128, 128])  # latin_hypercubes_2 needs number = prime number**2\n",
    "mu_list = [0, 10, 50, 75, 100, 200]\n",
    "use_random_eval_grids = False\n",
    "n_grid_points_eval = 10000\n",
    "prior_support = (-4.5, 3.5)\n",
    "n_iterations = 100\n",
    "n_individuals_mc = 1000\n",
    "\n",
    "if not use_random_eval_grids:\n",
    "    # use fixed grid\n",
    "    x = np.linspace(prior_support[0], prior_support[1], int(np.ceil(np.sqrt(n_grid_points_eval))))\n",
    "    y = np.linspace(prior_support[0], prior_support[1], int(np.ceil(np.sqrt(n_grid_points_eval))))\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z_real = eval_gaussian_grid(X, Y, gaussian_means=means, gaussian_covs=sigmas, gaussian_mixing_coeff=mixing_probs)\n",
    "\n",
    "error_mc = np.zeros((len(method_list), len(mu_list), n_iterations))\n",
    "moments_1_mc = np.zeros((len(method_list), len(mu_list), n_iterations, products.shape[1]))\n",
    "moments_2_mc = np.zeros((len(method_list), len(mu_list), n_iterations, products.shape[1]))\n",
    "error_moments_1_mc = np.zeros((len(method_list), len(mu_list), n_iterations, products.shape[1]))\n",
    "error_moments_2_mc = np.zeros((len(method_list), len(mu_list), n_iterations, products.shape[1]))\n",
    "for i in tqdm(range(n_iterations), desc='Computing Errors'):\n",
    "    # create new data\n",
    "    mc_products, beta_unobserved, mc_choiceData = generate_data(gaussian_means=means,\n",
    "                                                                gaussian_sigmas=sigmas,\n",
    "                                                                gaussian_mixing_probs=mixing_probs,\n",
    "                                                                n_consumers=n_individuals_mc,\n",
    "                                                                seed=i)\n",
    "    real_sample_mean = np.mean(beta_unobserved, axis=0)\n",
    "    real_sample_var = np.var(beta_unobserved, axis=0)\n",
    "\n",
    "    # eval grid\n",
    "    if use_random_eval_grids:\n",
    "        xy = make_beta_grid(n_grid_points=n_grid_points_eval,\n",
    "                            dim=mc_products.shape[2],\n",
    "                            support=prior_support,\n",
    "                            sampling_type='halton')\n",
    "        Z_real = eval_gaussian_vector(xy, gaussian_means=means, gaussian_covs=sigmas, gaussian_mixing_coeff=mixing_probs)\n",
    "\n",
    "    for method_i, method in enumerate(method_list):\n",
    "        beta_grid_test = make_beta_grid(n_grid_points=n_grid_points_list[method_i],\n",
    "                                        dim=mc_products.shape[2],\n",
    "                                        support=prior_support,\n",
    "                                        sampling_type=method)\n",
    "\n",
    "        for mu_i, mu_penalty in enumerate(mu_list):\n",
    "            theta_test, _ = solve_non_parametric(data_y=mc_choiceData,\n",
    "                                                 data_x=mc_products,\n",
    "                                                 grid=beta_grid_test,\n",
    "                                                 mu_penalty=mu_penalty,\n",
    "                                                 verbose=False)\n",
    "            # compute mean squared error to true cdf\n",
    "            if use_random_eval_grids:\n",
    "                Z_estimated = estimated_cdf_vector(xy, grid=beta_grid_test, thetas=theta_test)\n",
    "                error_mc[method_i, mu_i, i]  = np.mean((Z_real-Z_estimated)**2)\n",
    "            else:\n",
    "                Z_estimated = estimated_cdf_grid(X, Y, grid=beta_grid_test, thetas=theta_test)\n",
    "                error_mc[method_i, mu_i, i]  = np.mean((Z_real[:, :, 1]-Z_estimated)**2)\n",
    "\n",
    "            # compute error in moments\n",
    "            moments_1_mc[method_i, mu_i, i] = np.array([compute_moment(beta_grid_test, theta_test, n_moments=(1, 0)),\n",
    "                                                        compute_moment(beta_grid_test, theta_test, n_moments=(0, 1))])\n",
    "\n",
    "            moments_2_mc[method_i, mu_i, i] = np.array([compute_moment(beta_grid_test, theta_test, n_moments=(2, 0)),\n",
    "                                                        compute_moment(beta_grid_test, theta_test, n_moments=(0, 2))])\n",
    "            moments_2_mc[method_i, mu_i, i] -= moments_1_mc[method_i, mu_i, i]**2\n",
    "\n",
    "            # mean over different product dimensions\n",
    "            error_moments_1_mc[method_i, mu_i, i] = (moments_1_mc[method_i, mu_i, i] - real_sample_mean)**2\n",
    "            error_moments_2_mc[method_i, mu_i, i] = moments_2_mc[method_i, mu_i, i]/real_sample_var"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('error to true cdf')\n",
    "fig, ax = plt.subplots(1, len(mu_list), figsize=(10, 5), sharey='all', sharex='all', tight_layout=True)\n",
    "\n",
    "for mu_i in range(len(mu_list)):\n",
    "    ax[mu_i].boxplot(error_mc[:, mu_i, :].T, labels=method_list, vert=False)\n",
    "    ax[mu_i].set_title(f'$\\mu={mu_list[mu_i]}$')\n",
    "#plt.xscale('log')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take mean over product attributes\n",
    "error_moments_1_mc_mean = np.mean(error_moments_1_mc, axis=3)\n",
    "error_moments_2_mc_mean = np.mean(error_moments_2_mc, axis=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('squared difference in mean')\n",
    "fig, ax = plt.subplots(1, len(mu_list), figsize=(10, 6), sharey='all', sharex='all', tight_layout=True)\n",
    "for mu_i in range(len(mu_list)):\n",
    "    ax[mu_i].boxplot(error_moments_1_mc_mean[:, mu_i, :].T, labels=method_list, vert=False)\n",
    "    ax[mu_i].set_title(f'$\\mu={mu_list[mu_i]}$')\n",
    "#plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "print('relative estimated variance')\n",
    "fig, ax = plt.subplots(1, len(mu_list), figsize=(10, 6), sharey='all', sharex='all', tight_layout=True)\n",
    "for mu_i in range(len(mu_list)):\n",
    "    ax[mu_i].boxplot(error_moments_2_mc_mean[:, mu_i, :].T, labels=method_list, vert=False)\n",
    "    ax[mu_i].set_title(f'$\\mu={mu_list[mu_i]}$')\n",
    "#plt.xscale('log')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take median over iterations of monte carlo simulation\n",
    "for i, rel_err_var in enumerate(np.median(error_moments_2_mc_mean, axis=2)):\n",
    "    plt.plot(mu_list, rel_err_var, label=method_list[i])\n",
    "    #plt.plot(mu_list, (rel_err_var-7)/4, label=method_list[i])\n",
    "plt.xlabel(\"$\\mu$\")\n",
    "plt.ylabel(\"Median of Relative Error of Variance\")\n",
    "#plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monte Carlo Simulations with Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "method_list = ['uniform', 'latin_hypercubes', 'latin_hypercubes_2', 'sobol', 'halton']\n",
    "n_grid_points_list = np.array([128, 128, 121, 128, 128])  # latin_hypercubes_2 needs number = prime number**2\n",
    "use_random_eval_grids = False\n",
    "n_grid_points_eval = 10000\n",
    "prior_support = (-4.5, 3.5)\n",
    "n_iterations = 100\n",
    "k_folds = 2\n",
    "n_individuals_mc = 1000\n",
    "\n",
    "if not use_random_eval_grids:\n",
    "    # use fixed grid\n",
    "    x = np.linspace(prior_support[0], prior_support[1], int(np.ceil(np.sqrt(n_grid_points_eval))))\n",
    "    y = np.linspace(prior_support[0], prior_support[1], int(np.ceil(np.sqrt(n_grid_points_eval))))\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z_real = eval_gaussian_grid(X, Y, gaussian_means=means, gaussian_covs=sigmas, gaussian_mixing_coeff=mixing_probs)\n",
    "\n",
    "error_mc = np.zeros((len(method_list), 3, n_iterations))\n",
    "moments_1_mc = np.zeros((len(method_list), 3, n_iterations, products.shape[1]))\n",
    "moments_2_mc = np.zeros((len(method_list), 3, n_iterations, products.shape[1]))\n",
    "error_moments_1_mc = np.zeros((len(method_list), 3, n_iterations, products.shape[1]))\n",
    "error_moments_2_mc = np.zeros((len(method_list), 3, n_iterations, products.shape[1]))\n",
    "list_mu_MSE = np.zeros((len(method_list), n_iterations))\n",
    "list_mu_OneSE = np.zeros((len(method_list), n_iterations))\n",
    "for i in tqdm(range(n_iterations), desc='Computing Errors'):\n",
    "    # create new data\n",
    "    mc_products, beta_unobserved, mc_choiceData = generate_data(gaussian_means=means,\n",
    "                                                                gaussian_sigmas=sigmas,\n",
    "                                                                gaussian_mixing_probs=mixing_probs,\n",
    "                                                                n_consumers=n_individuals_mc,\n",
    "                                                                seed=i)\n",
    "    real_sample_mean = np.mean(beta_unobserved, axis=0)\n",
    "    real_sample_var = np.var(beta_unobserved, axis=0)\n",
    "\n",
    "    # create random mu for cross validation, using halton sequence to get more or less equidistant points\n",
    "    mu_list_cv_test = make_beta_grid(n_grid_points=18,\n",
    "                            dim=1,\n",
    "                            support=(0,100),\n",
    "                            sampling_type='halton').flatten()\n",
    "    mu_list_cv_test = np.append(mu_list_cv_test, [0,100])\n",
    "    mu_list_cv_test.sort()\n",
    "\n",
    "    # eval grid\n",
    "    if use_random_eval_grids:\n",
    "        xy = make_beta_grid(n_grid_points=n_grid_points_eval,\n",
    "                            dim=mc_products.shape[2],\n",
    "                            support=prior_support,\n",
    "                            sampling_type='halton')\n",
    "        Z_real = eval_gaussian_vector(xy, gaussian_means=means, gaussian_covs=sigmas, gaussian_mixing_coeff=mixing_probs)\n",
    "\n",
    "    for method_i, method in enumerate(method_list):\n",
    "        beta_grid_test = make_beta_grid(n_grid_points=n_grid_points_list[method_i],\n",
    "                                        dim=mc_products.shape[2],\n",
    "                                        support=prior_support,\n",
    "                                        sampling_type=method)\n",
    "\n",
    "        weights_list, cross_validation_error = cross_validate(data_x=mc_products,\n",
    "                                                              data_y=mc_choiceData,\n",
    "                                                              mu_list=mu_list_cv_test,\n",
    "                                                              beta_grid=beta_grid_test,\n",
    "                                                              kfold=k_folds,\n",
    "                                                              verbose=False)\n",
    "\n",
    "        mu_MSE, mu_OneSe = compute_cv_mu(mu_list_cv_test, cross_validation_error)\n",
    "        list_mu_MSE[method_i, i] = mu_MSE\n",
    "        list_mu_OneSE[method_i, i] = mu_OneSe\n",
    "\n",
    "        for mu_i, mu_penalty in enumerate([0, mu_MSE, mu_OneSe]):\n",
    "            theta_test, _ = solve_non_parametric(data_y=mc_choiceData,\n",
    "                                                 data_x=mc_products,\n",
    "                                                 grid=beta_grid_test,\n",
    "                                                 mu_penalty=mu_penalty,\n",
    "                                                 verbose=False)\n",
    "            # compute mean squared error to true cdf\n",
    "            if use_random_eval_grids:\n",
    "                Z_estimated = estimated_cdf_vector(xy, grid=beta_grid_test, thetas=theta_test)\n",
    "                error_mc[method_i, mu_i, i]  = np.mean((Z_real-Z_estimated)**2)\n",
    "            else:\n",
    "                Z_estimated = estimated_cdf_grid(X, Y, grid=beta_grid_test, thetas=theta_test)\n",
    "                error_mc[method_i, mu_i, i]  = np.mean((Z_real[:, :, 1]-Z_estimated)**2)\n",
    "\n",
    "            # compute error in moments\n",
    "            moments_1_mc[method_i, mu_i, i] = np.array([compute_moment(beta_grid_test, theta_test, n_moments=(1, 0)),\n",
    "                                                        compute_moment(beta_grid_test, theta_test, n_moments=(0, 1))])\n",
    "\n",
    "            moments_2_mc[method_i, mu_i, i] = np.array([compute_moment(beta_grid_test, theta_test, n_moments=(2, 0)),\n",
    "                                                        compute_moment(beta_grid_test, theta_test, n_moments=(0, 2))])\n",
    "            moments_2_mc[method_i, mu_i, i] -= moments_1_mc[method_i, mu_i, i]**2\n",
    "\n",
    "            # mean over different product dimensions\n",
    "            error_moments_1_mc[method_i, mu_i, i] = (moments_1_mc[method_i, mu_i, i] - real_sample_mean)**2\n",
    "            error_moments_2_mc[method_i, mu_i, i] = moments_2_mc[method_i, mu_i, i]/real_sample_var"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu_name = ['0', '$\\mu_{MSE}$', '$\\mu_{OneSE}$']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('error to true cdf')\n",
    "fig, ax = plt.subplots(1, len(mu_name), figsize=(10, 5), sharey='all', sharex='all', tight_layout=True)\n",
    "\n",
    "for mu_i in range(len(mu_name)):\n",
    "    ax[mu_i].boxplot(error_mc[:, mu_i, :].T, labels=method_list, vert=False)\n",
    "    ax[mu_i].set_title(f'{mu_name[mu_i]}')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "error_moments_1_mc_mean = np.mean(error_moments_1_mc, axis=3)\n",
    "error_moments_2_mc_mean = np.mean(error_moments_2_mc, axis=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('squared difference in mean')\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 6), sharey='all', sharex='all', tight_layout=True)\n",
    "for mu_i in range(len(mu_name)):\n",
    "    ax[mu_i].boxplot(error_moments_1_mc_mean[:, mu_i, :].T, labels=method_list, vert=False)\n",
    "    ax[mu_i].set_title(f'{mu_name[mu_i]}')\n",
    "#plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "print('relative estimated variance')\n",
    "fig, ax = plt.subplots(1, len(mu_name), figsize=(10, 6), sharey='all', sharex='all', tight_layout=True)\n",
    "for mu_i in range(len(mu_name)):\n",
    "    ax[mu_i].boxplot(error_moments_2_mc_mean[:, mu_i, :].T, labels=method_list, vert=False)\n",
    "    ax[mu_i].set_title(f'{mu_name[mu_i]}')\n",
    "#plt.xscale('log')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('different choices of $\\mu$ of cross-validation')\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharey='all', sharex='all', tight_layout=True)\n",
    "\n",
    "ax[0].boxplot(list_mu_MSE.T, labels=method_list, vert=False)\n",
    "ax[0].set_title(f'{mu_name[1]}')\n",
    "ax[1].boxplot(list_mu_OneSE.T, labels=method_list, vert=False)\n",
    "ax[1].set_title(f'{mu_name[2]}')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
